AWSTemplateFormatVersion: '2010-09-09'
Description: 'Install Karpenter on existing EKS cluster'

Parameters:
  EKSClusterName:
    Type: String
    Description: Name of the EKS cluster
  PrivateSubnets:
    Type: CommaDelimitedList
    Description: Private subnet IDs for Karpenter nodes
  ClusterSecurityGroupId:
    Type: String
    Description: EKS cluster security group ID

Resources:
  # Create Karpenter IAM role for service account
  KarpenterControllerRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'KarpenterController-${EKSClusterName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: eks.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Ref KarpenterControllerPolicy

  KarpenterControllerPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub 'KarpenterController-${EKSClusterName}'
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action: '*'
            Resource: '*'

  # Karpenter Node IAM Role
  KarpenterNodeRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub 'KarpenterNode-${EKSClusterName}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  KarpenterNodeInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      InstanceProfileName: !Sub 'KarpenterNode-${EKSClusterName}'
      Roles:
        - !Ref KarpenterNodeRole

  # Lambda function to tag resources only
  KarpenterInstaller:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'KarpenterInstaller-${EKSClusterName}'
      Runtime: python3.9
      Handler: index.handler
      Role: !GetAtt KarpenterInstallerRole.Arn
      Timeout: 60
      Code:
        ZipFile: |
          import boto3
          import json
          import cfnresponse
          
          def handler(event, context):
              try:
                  if event['RequestType'] == 'Create':
                      cluster_name = event['ResourceProperties']['ClusterName']
                      subnets = event['ResourceProperties']['Subnets']
                      security_group = event['ResourceProperties']['SecurityGroup']
                      region = event['ResourceProperties']['Region']
                      
                      ec2 = boto3.client('ec2', region_name=region)
                      
                      # Tag subnets for Karpenter discovery
                      for subnet in subnets:
                          ec2.create_tags(
                              Resources=[subnet],
                              Tags=[{'Key': 'karpenter.sh/discovery', 'Value': cluster_name}]
                          )
                      
                      # Tag security group for Karpenter discovery
                      ec2.create_tags(
                          Resources=[security_group],
                          Tags=[{'Key': 'karpenter.sh/discovery', 'Value': cluster_name}]
                      )
                      
                      # Trigger CodeBuild to install Karpenter
                      codebuild = boto3.client('codebuild', region_name=region)
                      project_name = event['ResourceProperties']['ProjectName']
                      
                      response = codebuild.start_build(projectName=project_name)
                      build_id = response['build']['id']
                      
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                          'Message': f'Karpenter installation started via CodeBuild. Build ID: {build_id}'
                      })
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {'Message': 'No action required'})
              except Exception as e:
                  print('Error: ' + str(e))
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})

  KarpenterInstallerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: KarpenterTriggerPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ec2:CreateTags
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                  - codebuild:StartBuild
                Resource: '*'

  # CodeBuild project to install Karpenter
  KarpenterInstallProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Name: !Sub 'KarpenterInstall-${EKSClusterName}'
      ServiceRole: !GetAtt CodeBuildRole.Arn
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        Type: LINUX_CONTAINER
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/amazonlinux2-x86_64-standard:5.0
        EnvironmentVariables:
          - Name: CLUSTER_NAME
            Value: !Ref EKSClusterName
          - Name: AWS_DEFAULT_REGION
            Value: !Ref AWS::Region
          - Name: AWS_ACCOUNT_ID
            Value: !Ref AWS::AccountId
      Source:
        Type: NO_SOURCE
        BuildSpec: |
          version: 0.2
          phases:
            install:
              runtime-versions:
                python: 3.9
              commands:
                - echo "Installing kubectl and helm..."
                - curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
                - chmod +x kubectl && mv kubectl /usr/local/bin/
                - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
            pre_build:
              commands:
                - echo "Updating kubeconfig..."
                - aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_DEFAULT_REGION --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/KarpenterController-$CLUSTER_NAME
            build:
              commands:
                - echo "Installing Karpenter v1.0.1..."
                - helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version 1.0.1 --namespace karpenter --create-namespace --set settings.clusterName=$CLUSTER_NAME --wait
                - echo "Applying NodePool configuration..."
                - |
                  cat << EOF | kubectl apply -f -
                  apiVersion: karpenter.sh/v1
                  kind: NodePool
                  metadata:
                    name: default
                  spec:
                    template:
                      spec:
                        requirements:
                          - key: kubernetes.io/arch
                            operator: In
                            values: ["amd64", "arm64"]
                          - key: karpenter.sh/capacity-type
                            operator: In
                            values: ["spot", "on-demand"]
                        nodeClassRef:
                          apiVersion: karpenter.k8s.aws/v1
                          kind: EC2NodeClass
                          name: default
                    limits:
                      cpu: 1000
                  ---
                  apiVersion: karpenter.k8s.aws/v1
                  kind: EC2NodeClass
                  metadata:
                    name: default
                  spec:
                    amiFamily: AL2
                    subnetSelectorTerms:
                      - tags:
                          karpenter.sh/discovery: "$CLUSTER_NAME"
                    securityGroupSelectorTerms:
                      - tags:
                          karpenter.sh/discovery: "$CLUSTER_NAME"
                  EOF
            post_build:
              commands:
                - echo "Verifying Karpenter installation..."
                - kubectl get pods -n karpenter
                - kubectl get nodepool

  CodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: codebuild.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/CodeBuildServiceRolePolicy
      Policies:
        - PolicyName: EKSAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - eks:*
                  - ec2:*
                  - iam:*
                  - logs:*
                Resource: '*'

  # Custom resource to trigger setup
  KarpenterSetup:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt KarpenterInstaller.Arn
      ClusterName: !Ref EKSClusterName
      Subnets: !Ref PrivateSubnets
      SecurityGroup: !Ref ClusterSecurityGroupId
      Region: !Ref AWS::Region
      ProjectName: !Ref KarpenterInstallProject

Outputs:
  KarpenterControllerRoleArn:
    Description: Karpenter Controller Role ARN
    Value: !GetAtt KarpenterControllerRole.Arn
  KarpenterNodeRoleArn:
    Description: Karpenter Node Role ARN  
    Value: !GetAtt KarpenterNodeRole.Arn
  KarpenterSetupMessage:
    Description: Karpenter installation command
    Value: !GetAtt KarpenterSetup.Message